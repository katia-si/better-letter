{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carstenvolland/.pyenv/versions/3.10.6/envs/better-letter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Ist-oder-soll.jpeg. Output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text/Ist-oder-soll.txt already exists.\n",
      "Skipping 1000014342.jpg. Output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text/1000014342.txt already exists.\n",
      "Skipping letter_school_hamburg.png. Output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text/letter_school_hamburg.txt already exists.\n",
      "Skipping 1000014343.jpg. Output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text/1000014343.txt already exists.\n",
      "Skipping letter_arbeitsamt_internet.jpeg. Output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text/letter_arbeitsamt_internet.txt already exists.\n"
     ]
    }
   ],
   "source": [
    "### This part scans an image of letter and applies an OCR text recognition in order\n",
    "### to get the text of the image and transforms it to txt and stores it\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "\n",
    "def process_images(image_directory, output_directory):\n",
    "    \"\"\"\n",
    "    Process all images in the specified directory using OCR and save the extracted text to text files.\n",
    "\n",
    "    Args:\n",
    "        image_directory (str): Path to the directory containing the image files.\n",
    "        output_directory (str): Path to the directory where the output text files will be saved.\n",
    "    \"\"\"\n",
    "    # initialize the OCR reader\n",
    "    reader = easyocr.Reader(['de'])\n",
    "\n",
    "    # create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # iterate through all files in the directory\n",
    "    for filename in os.listdir(image_directory):\n",
    "        # check if file is an image\n",
    "        if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png') or filename.endswith('.pdf'):\n",
    "            # make the full path to the image file\n",
    "            image_path = os.path.join(image_directory, filename)\n",
    "\n",
    "            # open image for OCR\n",
    "            try:\n",
    "                with open(image_path, 'rb') as image_file:\n",
    "                    image_bytes = image_file.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # extract text from image using easyocr\n",
    "            try:\n",
    "                result = reader.readtext(image_bytes)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # extracted text\n",
    "            text = '\\n'.join([entry[1] for entry in result])\n",
    "\n",
    "            # define output path to save extracted text\n",
    "            output_path = os.path.join(output_directory, f'{os.path.splitext(filename)[0]}.txt')\n",
    "\n",
    "            # check if output file already exists\n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"Skipping {filename}. Output file {output_path} already exists.\")\n",
    "                continue\n",
    "\n",
    "            # save extracted text to a text file\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(text)\n",
    "\n",
    "            # print a confirmation message about the text extraction\n",
    "            print(f'Text from {filename} has been saved in: {output_path}.')\n",
    "\n",
    "# define path to the directory containing the image files\n",
    "image_directory = '/Users/carstenvolland/code/katia-si/better-letter/text_extractor/data_images/'\n",
    "\n",
    "# define the path to the directory where the output text files will be saved\n",
    "output_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text'\n",
    "\n",
    "# process images in the specified directory\n",
    "process_images(image_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping 1000014343.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/1000014343_cldn.txt already exists.\n",
      "skipping 1000014342.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/1000014342_cldn.txt already exists.\n",
      "skipping letter_school_hamburg.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/letter_school_hamburg_cldn.txt already exists.\n",
      "skipping Ist-oder-soll.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/Ist-oder-soll_cldn.txt already exists.\n",
      "skipping letter_arbeitsamt_internet.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/letter_arbeitsamt_internet_cldn.txt already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    # remove hyphens at line breaks, replace newlines with spaces, and remove extra spaces\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r';', ',', text)\n",
    "    # find the index of the first occurrence of \"Datum:\"\n",
    "    datum_index = text.find(\"Datum:\")\n",
    "    if datum_index != -1:\n",
    "        # remove text before \"Datum:\"\n",
    "        text = text[datum_index+len(\"Datum:\"):]\n",
    "    sehr_geehrt_index = re.search(r'Sehr geehrt.*', text)\n",
    "    if sehr_geehrt_index:\n",
    "        # remove text before \"Sehr geehrt\" followed by any characters\n",
    "        text = text[sehr_geehrt_index.start():]\n",
    "    return text\n",
    "\n",
    "def clean_and_save_files(input_directory, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            input_file_path = os.path.join(input_directory, filename)\n",
    "            output_file_path = os.path.join(output_directory, filename.replace('.txt', '_cldn.txt'))\n",
    "\n",
    "            if os.path.exists(output_file_path):\n",
    "                print(f\"skipping {filename}. output file {output_file_path} already exists.\")\n",
    "                continue\n",
    "\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                extracted_text = file.read()\n",
    "\n",
    "            cleaned_text = clean_extracted_text(extracted_text)\n",
    "\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(cleaned_text)\n",
    "\n",
    "            print(f'cleaned text has been saved to: {output_file_path}')\n",
    "\n",
    "# define the input directory containing the extracted text files\n",
    "input_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text'\n",
    "\n",
    "# define the output directory where cleaned text files will be saved\n",
    "output_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned'\n",
    "\n",
    "# clean and save the files\n",
    "clean_and_save_files(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### summarize german text short\n",
    "#\n",
    "# from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "# import os\n",
    "#\n",
    "# # load pre-trained model and tokenizer\n",
    "# model_name = \"facebook/bart-large-cnn\"\n",
    "# tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "# model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "#\n",
    "# # function to generate summary\n",
    "# def generate_summary(text):\n",
    "#     inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
    "#     summary_ids = model.generate(inputs['input_ids'], num_beams=4, min_length=30, max_length=200, early_stopping=True)\n",
    "#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "#     return summary\n",
    "#\n",
    "# # define input and output directories\n",
    "# input_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/english_translation'\n",
    "# output_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary'\n",
    "#\n",
    "# create the output directory if it doesn't exist\n",
    "# if not os.path.exists(output_directory):\n",
    "#     os.makedirs(output_directory)\n",
    "#\n",
    "# # generate summary for each file in the input directory\n",
    "# for filename in os.listdir(input_directory):\n",
    "#     # check if file is a text file\n",
    "#     if filename.endswith('.txt'):\n",
    "#         # construct the full path to the input file\n",
    "#         input_file_path = os.path.join(input_directory, filename)\n",
    "#\n",
    "#         # construct the full path to the output file\n",
    "#         output_file_path = os.path.join(output_directory, f'{os.path.splitext(filename)[0]}_sum.txt')\n",
    "#\n",
    "#         # check if the output file already exists\n",
    "#         if os.path.exists(output_file_path):\n",
    "#             print(f'skipping {filename}. output file {output_file_path} already exists.')\n",
    "#             continue\n",
    "#\n",
    "#         # read the content of the input file\n",
    "#         with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "#             text = file.read()\n",
    "#\n",
    "#         # generate summary\n",
    "#         summary = generate_summary(text)\n",
    "#\n",
    "#         # write summary to the output file\n",
    "#         with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "#             output_file.write(summary)\n",
    "#\n",
    "#         print(f'summary has been generated and saved to: {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/letter_arbeitsamt_internet_cldn_sum.txt\n",
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/1000014342_cldn_sum.txt\n",
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/1000014343_cldn_sum.txt\n",
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/Ist-oder-soll_cldn_sum.txt\n",
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/letter_school_hamburg_cldn_sum.txt\n"
     ]
    }
   ],
   "source": [
    "### trying a longer summarization of the german text\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import os\n",
    "\n",
    "# load pre-trained BART model and tokenizer for German\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Shahm/bart-german\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Shahm/bart-german\")\n",
    "\n",
    "# function to generate summary with adjusted parameters\n",
    "def generate_summary_longer(text: str) -> str:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=500,  # increase max_length to make the summary longer\n",
    "        min_length=150,  # adjust min_length accordingly\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# define input and output directories\n",
    "input_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/'\n",
    "output_directory = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/'\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# generate summary for each file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    # check if file is a text file\n",
    "    if filename.endswith('.txt'):\n",
    "        # construct the full path to the input file\n",
    "        input_file_path = os.path.join(input_directory, filename)\n",
    "\n",
    "        # construct the full path to the output file\n",
    "        output_file_path = os.path.join(output_directory, f'{os.path.splitext(filename)[0]}_sum.txt')\n",
    "\n",
    "        # check if the output file already exists\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f'skipping {filename}. output file {output_file_path} already exists.')\n",
    "            continue\n",
    "\n",
    "        # read the content of the input file\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # generate summary\n",
    "        summary = generate_summary_longer(text)\n",
    "\n",
    "        # write summary to the output file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(summary)\n",
    "\n",
    "        print(f'summary has been generated and saved to: {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary_long/1000014343_cldn_sum.txt\n",
      "translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary_long/1000014342_cldn_sum.txt\n",
      "translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary_long/letter_school_hamburg_cldn_sum.txt\n",
      "translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary_long/letter_arbeitsamt_internet_cldn_sum.txt\n",
      "translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary_long/Ist-oder-soll_cldn_sum.txt\n"
     ]
    }
   ],
   "source": [
    "### translate german summarized text to english\n",
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import os\n",
    "\n",
    "# load the MarianMTModel and tokenizer for translation\n",
    "tokenizer_translate = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
    "model_translate = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
    "\n",
    "# function to translate German text to English\n",
    "def translate_to_english(german_text):\n",
    "    # tokenize the German text\n",
    "    inputs = tokenizer_translate(german_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # generate English translation\n",
    "    translated = model_translate.generate(**inputs)\n",
    "    # decode the translated text\n",
    "    translated_text = tokenizer_translate.batch_decode(translated, skip_special_tokens=True)\n",
    "    return translated_text[0]\n",
    "\n",
    "# define input and output directories for German summaries\n",
    "input_directory_german = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary_long/'\n",
    "output_directory_english = '/Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary_long/'\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory_english):\n",
    "    os.makedirs(output_directory_english)\n",
    "\n",
    "# translate and save summaries for each file in the input directory\n",
    "for filename in os.listdir(input_directory_german):\n",
    "    # check if file is a text file\n",
    "    if filename.endswith('.txt'):\n",
    "        # read the German summary\n",
    "        with open(os.path.join(input_directory_german, filename), 'r', encoding='utf-8') as file:\n",
    "            german_summary = file.read()\n",
    "\n",
    "        # translate German summary to English\n",
    "        english_summary = translate_to_english(german_summary)\n",
    "\n",
    "        # construct the full path to the output file\n",
    "        output_file_path_english = os.path.join(output_directory_english, filename)\n",
    "\n",
    "        # write the translated summary to the output file\n",
    "        with open(output_file_path_english, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(english_summary)\n",
    "\n",
    "        print(f'translated summary has been saved to: {output_file_path_english}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Differenz zwischen dem von den Eltern zu zahlenden Maximalpreis von 4,35 Euro und der neuen Preisobergrenze für die Caterer übernimmt die Schulbehörde. Das gilt für alle Mittagessen; auch die der vollzahlendsten Schülerinnen und Schülern. Damit setzt der Hamburger Senat konsequent die seit 2020 begonnene Linie durch. Mit freundlichen Grüßen Ihr Ganzirksame Ganztagsreferat pro state of the sun: Die Freie und Hansestadt Hamburg.\n"
     ]
    }
   ],
   "source": [
    "### playground to test several paramters in order to get the best translation\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load BART model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Shahm/bart-german\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Shahm/bart-german\")\n",
    "\n",
    "# Define your original text\n",
    "original_text = \"\"\"\n",
    "Schulisches Mittagessen: Neuer Maximalpreis für das Schuljahr 2023/24 und neue Preisobergrenze für Caterer ab dem 01.08.2023 Sehr geehrte Eltern, mit diesem Schreiben möchten wir Sie darüber informieren; dass auf Basis der allgemeinen Preisentwicklung der von den Eltern zu zahlende Höchstpreis für das Mittagessen zum 1. August 2023 um 20 Cent auf 4,35 Euro angehoben wird. Angesichts der nach wie vor hohen Lebensmittelpreise deckt dieser Beitrag derzeit nicht die tatsächlichen Kosten für ein Mittagessen: Die Caterer können daher ab dem 01.08.2023 bis zu 4,80 Euro Mittagessen abrechnen Die Differenz zwischen dem von den Eltern zu zahlenden Maximalpreis von 4,35 Euro und der neuen Preisobergrenze für die Caterer übernimmt die Schulbehörde und rechnet die Differenz direkt mit den Caterern ab. Das gilt für alle Mittagessen; auch die der vollzahlenden Schülerinnen und Schüler. Damit setzt der Hamburger Senat konsequent die seit 2020 begonnene Linie durch; im Sinne der Familien; Kinder und Jugendlichen ein schulisches Mittagessen zu vertretbaren Preisen zu sichern und den an den Hamburger Schulen tätigen Cateringunternehmen angemessene Preise zu ermöglichen: Insgesamt übernehmen die Freie und Hansestadt Hamburg und der Bund deutlich mehr als 50 Prozent der Kosten aller schulischen Mittagessen, um für alle Schülerinnen und Schüler an Hamburgs Schulen ein gesundes Mittagessen zu gewährleisten: Mehr Informationen hierzu finden Sie bei Bedarf unter Mittaqessen_für_die_Hamburqer_Schulen hamburq de. Mit freundlichen Grüßen Ihr Ganztagsreferat pro\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary with longer max_length\n",
    "def generate_summary_longer(text: str) -> str:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=400,  # Increase max_length to make the summary longer\n",
    "        min_length=150,  # Adjust min_length accordingly\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Generate longer summary\n",
    "summary_longer = generate_summary_longer(original_text)\n",
    "\n",
    "# Print the longer summary\n",
    "print(summary_longer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Umsatzsteuer für ab diesem Zeitpunkt getätigte Umsätze nach vereinnahmten Entgelten zu berechnen. Bei der Erlaubnis geht das Finanzamt davon aus, dass Sie insoweit aus einer keit als Angehöriger eines freien Berufes im Sinne des $ 18 Abs. Nr. des Einkommensteuergesetzes ausführen.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "def summarize_with_bart(text):\n",
    "    # Laden des Modells und des Tokenizers\n",
    "    model_name = \"shahm/bart-german\"\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenisierung des Textes\n",
    "    inputs = tokenizer([text], max_length=2056, return_tensors='pt', truncation=True)\n",
    "\n",
    "    # Zusammenfassung generieren\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, min_length=100, max_length=400, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Beispieltext\n",
    "german_text = \"\"\"\n",
    "Sehr geehrte Damen und Herren, auf Ihren Antrag vom 24.06.2021 gestatte ich der im Betreff genannten Unternehmerin gemäß $ 20 des Umsatzsteuergesetzes (UStG) unter dem Vorbehalt des jederzeitigen Widerrufs, ab 01.06.2021 die Umsatzsteuer für ab diesem Zeitpunkt getätigte Umsätze nach vereinnahmten Entgelten zu berechnen. Bei der Erlaubnis geht das Finanzamt davon aus, dass Sie insoweit Umsätze aus einer keit als Angehöriger eines freien Berufes im Sinne des $ 18 Abs. Nr. des Einkommensteuergesetzes ausführen, für diese Tätigkeit weder Grund einer gesetzlichen Verpflichtung noch freiwillig Bücher führen und daher - unabhängig von der Umsatzgrenze von 600.000 Euro die Berechnung der Umsatzsteuer nach vereinnahmten Entgelten erfolgen kann S 20 Abs Nr. 3 UStG). Falls die 0.g. Voraussetzungen bei der im Betreff genannten Firma künftig wegfallen sollten, besteht ohne besondere Aufforderung die Verpflichtung, vom Beginn des darauf folgenden Kalenderjahres an die Umsatzsteuer wieder nach vereinbarten Entgelten zu berechnen: Verkehrsverbindungen Sprechzeiten Kreditinstitut Berliner Sparkasse Postbank Berlin S-Bahn $ 8 S 85 5 41 Bitte beachten Sie die geander - IBAN DE94 1005 0O00 6600 0464 63 DEO9 1001 0010 0691 5551 00 (Ring) Landsberger Allee ten Öffnungszeiten wahrend der BIC BELADEBEXXX PBNKDEFF Bus Storkower Str. Coronapandemie Die aktuellen IEinkaufszentrum Offnungszeiten finden Sie unter Straßenbahn MS, M6 MB $ W berin de Internet www berlin delsenlfinanzen Landsberger Allee Telefax (030) 9024-28900 Tätigauf\n",
    "\"\"\"\n",
    "\n",
    "# Zusammenfassung erstellen\n",
    "summary = summarize_with_bart(german_text)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
