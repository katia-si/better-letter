{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Ist-oder-soll.jpeg. Output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text/Ist-oder-soll.txt already exists.\n",
      "Skipping 1000014342.jpg. Output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text/1000014342.txt already exists.\n",
      "Skipping letter_school_hamburg.png. Output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text/letter_school_hamburg.txt already exists.\n",
      "Skipping 1000014343.jpg. Output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text/1000014343.txt already exists.\n",
      "Skipping letter_arbeitsamt_internet.jpeg. Output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text/letter_arbeitsamt_internet.txt already exists.\n"
     ]
    }
   ],
   "source": [
    "### This part scans an image of letter and applies an OCR text recognition in order\n",
    "### to get the text of the image and transforms it to txt and stores it\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "\n",
    "\n",
    "def process_images(image_directory, output_directory):\n",
    "    \"\"\"\n",
    "    Process all images in the specified directory using OCR and save the extracted text to text files.\n",
    "\n",
    "    Args:\n",
    "        image_directory (str): Path to the directory containing the image files.\n",
    "        output_directory (str): Path to the directory where the output text files will be saved.\n",
    "    \"\"\"\n",
    "    # initialize the OCR reader\n",
    "    reader = easyocr.Reader(['de'])\n",
    "\n",
    "    # create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # iterate through all files in the directory\n",
    "    for filename in os.listdir(image_directory):\n",
    "        # check if file is an image\n",
    "        if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png') or filename.endswith('.pdf'):\n",
    "            # make the full path to the image file\n",
    "            image_path = os.path.join(image_directory, filename)\n",
    "\n",
    "            # open image for OCR\n",
    "            try:\n",
    "                with open(image_path, 'rb') as image_file:\n",
    "                    image_bytes = image_file.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # extract text from image using easyocr\n",
    "            try:\n",
    "                result = reader.readtext(image_bytes)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # extracted text\n",
    "            text = '\\n'.join([entry[1] for entry in result])\n",
    "\n",
    "            # define output path to save extracted text\n",
    "            output_path = os.path.join(output_directory, f'{os.path.splitext(filename)[0]}.txt')\n",
    "\n",
    "            # check if output file already exists\n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"Skipping {filename}. Output file {output_path} already exists.\")\n",
    "                continue\n",
    "\n",
    "            # save extracted text to a text file\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(text)\n",
    "\n",
    "            # print a confirmation message about the text extraction\n",
    "            print(f'Text from {filename} has been saved in: {output_path}.')\n",
    "\n",
    "# get the directory of the current script\n",
    "current_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "# define path to the directory containing the image files\n",
    "image_directory = os.path.join(current_directory, 'text_extractor', 'data_images')\n",
    "\n",
    "# define the path to the directory where the output text files will be saved\n",
    "output_directory = os.path.join(current_directory, 'raw_data', 'german_ocr_text')\n",
    "\n",
    "# process images in the specified directory\n",
    "process_images(image_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping 1000014343.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/1000014343_cldn.txt already exists.\n",
      "skipping 1000014342.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/1000014342_cldn.txt already exists.\n",
      "skipping letter_school_hamburg.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/letter_school_hamburg_cldn.txt already exists.\n",
      "skipping Ist-oder-soll.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/Ist-oder-soll_cldn.txt already exists.\n",
      "skipping letter_arbeitsamt_internet.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_ocr_text_cleaned/letter_arbeitsamt_internet_cldn.txt already exists.\n"
     ]
    }
   ],
   "source": [
    "### data cleaning of the ocr scans to remove indentation and hyphenation and so on\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    # remove hyphens at line breaks, replace newlines with spaces, and remove extra spaces\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r';', ',', text)\n",
    "    text = re.sub(r':', '.', text)\n",
    "\n",
    "    # remove text inside curly or other brackets\n",
    "    text = re.sub(r'\\{.*?\\}', '', text)\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "    # find the index of the first occurrence of \"Datum:\"\n",
    "    datum_index = text.find(\"Datum:\")\n",
    "    if datum_index != -1:\n",
    "        # remove text before \"Datum:\"\n",
    "        text = text[datum_index+len(\"Datum:\"):]\n",
    "    sehr_geehrt_index = re.search(r'Sehr geehrt.*', text, flags=re.IGNORECASE)\n",
    "    if sehr_geehrt_index:\n",
    "        # remove text before \"Sehr geehrt\" followed by any characters\n",
    "        text = text[sehr_geehrt_index.start():]\n",
    "\n",
    "    # find the index of \"Berliner Sparkasse\" and remove everything after it\n",
    "    berliner_sparkasse_index = text.find(\"Berliner Sparkasse\")\n",
    "    if berliner_sparkasse_index != -1:\n",
    "        text = text[:berliner_sparkasse_index]\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_and_save_files(input_directory, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            input_file_path = os.path.join(input_directory, filename)\n",
    "            output_file_path = os.path.join(output_directory, filename.replace('.txt', '_cldn.txt'))\n",
    "\n",
    "            if os.path.exists(output_file_path):\n",
    "                print(f\"skipping {filename}. output file {output_file_path} already exists.\")\n",
    "                continue\n",
    "\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                extracted_text = file.read()\n",
    "\n",
    "            cleaned_text = clean_extracted_text(extracted_text)\n",
    "\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(cleaned_text)\n",
    "\n",
    "            print(f'cleaned text has been saved to: {output_file_path}')\n",
    "\n",
    "# get current working directory\n",
    "current_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "# define input directory containing the extracted text files\n",
    "input_directory = os.path.join(current_directory, 'raw_data', 'german_ocr_text')\n",
    "\n",
    "# define  output directory where cleaned text files will be saved\n",
    "output_directory = os.path.join(current_directory, 'raw_data', 'german_ocr_text_cleaned')\n",
    "\n",
    "# clean and save  files\n",
    "clean_and_save_files(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary has been generated and saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary/letter_arbeitsamt_internet_cldn_sum.txt\n",
      "skipping 1000014342_cldn.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary/1000014342_cldn_sum.txt already exists.\n",
      "skipping 1000014343_cldn.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary/1000014343_cldn_sum.txt already exists.\n",
      "skipping Ist-oder-soll_cldn.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary/Ist-oder-soll_cldn_sum.txt already exists.\n",
      "skipping letter_school_hamburg_cldn.txt. output file /Users/carstenvolland/code/katia-si/better-letter/raw_data/german_summary/letter_school_hamburg_cldn_sum.txt already exists.\n"
     ]
    }
   ],
   "source": [
    "### trying a longer summarization of the german text\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import os\n",
    "\n",
    "# load pre-trained BART model and tokenizer for German\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Shahm/bart-german\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Shahm/bart-german\")\n",
    "\n",
    "# function to generate summary with adjusted parameters\n",
    "def generate_summary_longer(text: str) -> str:\n",
    "\n",
    "    # generate summary\n",
    "    summary_ids = model.generate(\n",
    "        max_length=575,\n",
    "        min_length=200,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# define input and output directories\n",
    "current_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "input_directory = os.path.join(current_directory, 'raw_data', 'german_ocr_text_cleaned')\n",
    "output_directory = os.path.join(current_directory, 'raw_data', 'german_summary')\n",
    "\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# generate summary for each file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    # check if file is a text file\n",
    "    if filename.endswith('.txt'):\n",
    "        # construct the full path to the input file\n",
    "        input_file_path = os.path.join(input_directory, filename)\n",
    "\n",
    "        # construct the full path to the output file\n",
    "        output_file_path = os.path.join(output_directory, f'{os.path.splitext(filename)[0]}_sum.txt')\n",
    "\n",
    "        # check if the output file already exists\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f'skipping {filename}. output file {output_file_path} already exists.')\n",
    "            continue\n",
    "\n",
    "        # read the content of the input file\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # generate summary\n",
    "        summary = generate_summary_longer(text)\n",
    "\n",
    "        # write summary to the output file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(summary)\n",
    "\n",
    "        print(f'summary has been generated and saved to: {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary/1000014343_cldn_sum.txt\n",
      "translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary/1000014342_cldn_sum.txt\n",
      "translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary/letter_school_hamburg_cldn_sum.txt\n",
      "translated summary has been saved to: /Users/carstenvolland/code/katia-si/better-letter/raw_data/english_summary/Ist-oder-soll_cldn_sum.txt\n"
     ]
    }
   ],
   "source": [
    "### translate german summarized text to english\n",
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import os\n",
    "\n",
    "# load the MarianMTModel and tokenizer for translation\n",
    "tokenizer_translate = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
    "model_translate = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
    "\n",
    "# function to translate German text to English\n",
    "def translate_to_english(german_text):\n",
    "    # tokenize the German text\n",
    "    inputs = tokenizer_translate(german_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # generate English translation\n",
    "    translated = model_translate.generate(**inputs)\n",
    "    # decode the translated text\n",
    "    translated_text = tokenizer_translate.batch_decode(translated, skip_special_tokens=True)\n",
    "    return translated_text[0]\n",
    "\n",
    "# define input and output directories for english translation\n",
    "current_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "input_directory_german = os.path.join(current_directory, 'raw_data', 'german_summary')\n",
    "output_directory_english = os.path.join(current_directory, 'raw_data', 'english_summary')\n",
    "\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory_english):\n",
    "    os.makedirs(output_directory_english)\n",
    "\n",
    "# translate and save summaries for each file in the input directory\n",
    "for filename in os.listdir(input_directory_german):\n",
    "    # check if file is a text file\n",
    "    if filename.endswith('.txt'):\n",
    "        # read the German summary\n",
    "        with open(os.path.join(input_directory_german, filename), 'r', encoding='utf-8') as file:\n",
    "            german_summary = file.read()\n",
    "\n",
    "        # translate German summary to English\n",
    "        english_summary = translate_to_english(german_summary)\n",
    "\n",
    "        # construct the full path to the output file\n",
    "        output_file_path_english = os.path.join(output_directory_english, filename)\n",
    "\n",
    "        # write the translated summary to the output file\n",
    "        with open(output_file_path_english, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(english_summary)\n",
    "\n",
    "        print(f'translated summary has been saved to: {output_file_path_english}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Differenz zwischen dem von den Eltern zu zahlenden Maximalpreis von 4,35 Euro und der neuen Preisobergrenze übernimmt die Schulbehörde. Das gilt für alle Mittagessen, auch die der vollzahlendsten Schülerinnen und Schülern. Damit setzt der Hamburger Senat konsequent die seit 2020 begonnene Linie durch. Mehr Informationen hierzu finden Sie bei Bedarf unter Mittaqessen_... \"Hamburqer_Schulen hamburq de\". Ein Überblick überraschende Hamburger Hamburger Schulen, dass Cateringunternehmen angemessene Preise zu vertretbaren Preisen zu sichern. Die Kosten.\n"
     ]
    }
   ],
   "source": [
    "### playground to test several paramters in order to get the best translation\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load BART model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Shahm/bart-german\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Shahm/bart-german\")\n",
    "\n",
    "# Define your original text\n",
    "original_text = \"\"\"\n",
    "Sehr geehrte Eltern, mit diesem Schreiben möchten wir Sie darüber informieren, dass auf Basis der allgemeinen Preisentwicklung der von den Eltern zu zahlende Höchstpreis für das Mittagessen zum 1. August 2023 um 20 Cent auf 4,35 Euro angehoben wird. Angesichts der nach wie vor hohen Lebensmittelpreise deckt dieser Beitrag derzeit nicht die tatsächlichen Kosten für ein Mittagessen. Die Caterer können daher ab dem 01.08.2023 bis zu 4,80 Euro Mittagessen abrechnen Die Differenz zwischen dem von den Eltern zu zahlenden Maximalpreis von 4,35 Euro und der neuen Preisobergrenze für die Caterer übernimmt die Schulbehörde und rechnet die Differenz direkt mit den Caterern ab. Das gilt für alle Mittagessen, auch die der vollzahlenden Schülerinnen und Schüler. Damit setzt der Hamburger Senat konsequent die seit 2020 begonnene Linie durch, im Sinne der Familien, Kinder und Jugendlichen ein schulisches Mittagessen zu vertretbaren Preisen zu sichern und den an den Hamburger Schulen tätigen Cateringunternehmen angemessene Preise zu ermöglichen. Insgesamt übernehmen die Freie und Hansestadt Hamburg und der Bund deutlich mehr als 50 Prozent der Kosten aller schulischen Mittagessen, um für alle Schülerinnen und Schüler an Hamburgs Schulen ein gesundes Mittagessen zu gewährleisten. Mehr Informationen hierzu finden Sie bei Bedarf unter Mittaqessen_für_die_Hamburqer_Schulen hamburq de.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary with longer max_length\n",
    "def generate_summary_longer(text: str) -> str:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=575,\n",
    "        min_length=200,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Generate longer summary\n",
    "summary_longer = generate_summary_longer(original_text)\n",
    "\n",
    "# Print the longer summary\n",
    "print(summary_longer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
